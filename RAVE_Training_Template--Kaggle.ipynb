{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAVE\n",
    "**RAVE** is a variational autoencoder for fast and high-quality neural audio synthesis by Antoine Caillon and Philippe Esling.\n",
    "\\\n",
    "\\\n",
    "[Article on arxiv](https://arxiv.org/abs/2111.05011) & [Source code on Github](https://github.com/acids-ircam/RAVE)\n",
    "\n",
    "\\\n",
    "---\n",
    "\n",
    "<small>*This notebook has been written to set up a training with RAVE on kaggle.com using GPUs. Persistence should be set to \"Variables and Files\"*\n",
    "\\\n",
    "*Last updated: 10.03.2023*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Conda, RAVE, dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install Conda\n",
    "!curl -L https://repo.anaconda.com/miniconda/Miniconda3-py39_4.12.0-Linux-x86_64.sh -o miniconda.sh\n",
    "!chmod +x miniconda.sh\n",
    "!sh miniconda.sh -b -p /kaggle/working/miniconda\n",
    "\n",
    "#Install RAVE\n",
    "#Pin to specific release tag if necessary using \"acids-rave==tag.version\"\n",
    "!/kaggle/working/miniconda/bin/pip install acids-rave==2.1.12\n",
    "\n",
    "#Upgrade ipython ipykernel, install ffmpeg\n",
    "!/kaggle/working/miniconda/bin/pip install --upgrade pytorch_lightning ipython ipykernel\n",
    "!/kaggle/working/miniconda/bin/conda install ffmpeg --yes\n",
    "\n",
    "#Uninstall Cublas library. For some reason this creates conflicts with the environment on Kaggle. \n",
    "!/kaggle/working/miniconda/bin/pip uninstall nvidia_cublas_cu11 --yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the dataset and start initial training\n",
    "\n",
    "Preprocessing is necessary once per training set. In below section, the metadata from preprocessing is stored in '/kaggle/working/meta'. Use the filepath to your dataset on Kaggle as input path, this is usually '/kaggle/input/<yourdataset>'. \n",
    "\n",
    "While training the output is stored in a folder named <yourtrainingname> followed by an underscore and a 10-character-string inside '/kaggle/working/training/runs'. '/kaggle/working/training' also contains a 'status' folder with 'data.mdb' and 'lock.mdb' files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess dataset\n",
    "#Use flags \"--lazy\" when training on compressed dataset in various formats.\n",
    "%cd /kaggle/working\n",
    "!mkdir meta\n",
    "\n",
    "#Optional: you can resample your material to a fixed sample rate. Files will be store in a out_<selectedsamplerate> subfolder. Make sure to change your dataset folder in the section above and rerun it. \n",
    "#samplerate = \"44100\" #@param [\"22050\", \"44100\", \"48000\"]\n",
    "#resample --sr $samplerate --augment\n",
    "\n",
    "!/kaggle/working/miniconda/bin/rave preprocess --input_path /kaggle/input/<yourdataset>/ --output_path /kaggle/working/meta/\n",
    "\n",
    "#Initiate training\n",
    "%cd /kaggle/working\n",
    "!mkdir training\n",
    "%cd /kaggle/working/training\n",
    "\n",
    "#Use config parameters as preferred for your training. The setup below is exemplary.\n",
    "!/kaggle/working/miniconda/bin/rave train --config v2.gin --config wasserstein --db_path /kaggle/working/meta/ --name <yourtrainingname> --override PHASE_1_DURATION=100000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resume training\n",
    "In order to resume training RAVE on Kaggle, there are a few steps to take: \n",
    "1. Download **all relevant data** from your earlier training run you want to resume on (you can find this on the **output** tab)\n",
    "2. Create a **new dataset** and upload all data while **maintaining the folder structure** ('meta', 'runs' and 'status' folders incl. subfolders)\n",
    "3. Add dataset to the **notebook**. \n",
    "4. **Disable** the above initial **training section** of the notebook.\n",
    "\n",
    "***For initial training, the below section has been disabled, enable when you want to resume your training and disable the initial training section before running the notebook.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Copy contents of earlier training to /kaggle/working folder.\n",
    "#!cp -r /kaggle/input/<rootfolderofyourearliertraining>/* /kaggle/working\n",
    "\n",
    "#%cd /kaggle/working/training\n",
    "\n",
    "##Resume training\n",
    "##Use config parameters as used in your earlier training.\n",
    "#!/kaggle/working/miniconda/bin/rave train --config v2.gin --config wasserstein --db_path /kaggle/working/meta/ --name <yourtrainingname> --override PHASE_1_DURATION=100000 --ckpt /kaggle/working/training/runs/<yourtrainingnamewitharandomstringintheend>/version_<XX>/checkpoints/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create model\n",
    "After your training is finished, you can create a model (.ts Torchscript file). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model export. Use '--streaming' flag to export a model capable of real time processing.\n",
    "\n",
    "#!/kaggle/working/miniconda/bin/rave export --run /kaggle/working/training/runs/<yourtrainingnamewitharandomstringintheend> --streaming"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
