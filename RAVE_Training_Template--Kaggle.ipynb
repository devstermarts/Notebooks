{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# RAVE","metadata":{}},{"cell_type":"markdown","source":"**RAVE** is a variational autoencoder for fast and high-quality neural audio synthesis by Antoine Caillon and Philippe Esling.\n[Article on arxiv](https://arxiv.org/abs/2111.05011) & [Source code on Github](https://github.com/acids-ircam/RAVE)\n\n----\n\n<small>*Note that this notebook currently supports RAVE until version 2.2.2, which seems to be the most reliable.* \n\n\nNotebook Author: [Martin Heinze](https://github.com/devstermarts)\n\nDate: 23.11.2024","metadata":{}},{"cell_type":"markdown","source":"## Install Conda, RAVE, dependencies","metadata":{}},{"cell_type":"code","source":"#Install Miniconda\n!mkdir /kaggle/temp\n%cd /kaggle/temp\n!curl -L https://repo.anaconda.com/miniconda/Miniconda3-py39_4.12.0-Linux-x86_64.sh -o miniconda.sh\n!chmod +x miniconda.sh\n!sh miniconda.sh -b -p /kaggle/temp/miniconda\n\n#Upgrade ipython ipykernel, install ffmpeg\n!/kaggle/temp/miniconda/bin/pip install --upgrade ipython ipykernel\n!/kaggle/temp/miniconda/bin/conda install ffmpeg --yes","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Install RAVE\n#Pin to specific release tag if necessary using 'acids-rave==tag.versionâ€˜\n#Note that this notebook currently supports training RAVE until version 2.2.2.\n!/kaggle/temp/miniconda/bin/pip install acids-rave==2.2.2","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Preprocess the dataset\nPreprocessing is necessary once per training dataset. In below section, the data from preprocessing is stored in '/kaggle/working/processed'. Use the filepath to your dataset as input path, this is usually '/kaggle/input/your_audio_dataset'.","metadata":{}},{"cell_type":"code","source":"#Preprocess dataset\n!mkdir /kaggle/working/processed\n!/kaggle/temp/miniconda/bin/rave preprocess \\\n--input_path /kaggle/input/your_audio_dataset/ \\\n--output_path /kaggle/working/processed/","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Start initial training\nWhile training the output is stored in a folder named 'your_training_name' followed by an underscore and a 10-character-string inside '/kaggle/working/runs'. '/kaggle/working/' also contains a 'status' folder with 'data.mdb' and 'lock.mdb' files.\n\n***For initial training, the below section has been enabled, disable when you want to resume your training.***","metadata":{}},{"cell_type":"code","source":"#Initiate training\n#Use config parameters as preferred for your training. \n#The setup below is exemplary. \n#For configuration options check https://github.com/acids-ircam/RAVE#training\n#Other training parameters can be customized using --override, check train.py \n%cd /kaggle/working\n!/kaggle/temp/miniconda/bin/rave train \\\n--config v2.gin \\\n--config wasserstein \\\n--db_path /kaggle/working/processed/ \\\n--name your_training_name \\\n--override CAPACITY=128 \\\n--override PHASE_1_DURATION=2000000","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Resume training\nIn order to conveniently resume training RAVE on Kaggle, you can transform the output data of the first run into a new dataset from the output tab of your notebook. You then add the new dataset to the notebook, disable the above initial, **Preprocess the dataset** and **Start initial training** sections of the notebook and adjust the below section according to your initial training configuration.\n\nTo further progress on the training after the first run, update the processed dataset by creating a new version from the output of the latest run. Make sure to check for updates on the dataset in your notebook before resuming the training.\n\n***When you want to resume your training, disable the initial training section above before running the notebook.***","metadata":{}},{"cell_type":"code","source":"#Copy contents of earlier training to /kaggle/working folder.\n!cp -r /kaggle/input/root_folder_of_your_earlier_training/* /kaggle/working\n%cd /kaggle/working/\n\n#Resume training\n#Use config parameters as used in your earlier training.\n!/kaggle/temp/miniconda/bin/rave train \\\n--config v2.gin \\\n--config wasserstein \\\n--db_path /kaggle/working/processed/ \\\n--name your_training_name \\\n--override CAPACITY=128 \\\n--override PHASE_1_DURATION=2000000 \\\n--ckpt /kaggle/working/runs/your_training_name_with_a_random_string_in_the_end/version_X/checkpoints/","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Export model\nAfter your training is finished, you can export a model (.ts Torchscript) file.\n\n***Disable the initial and resume training sections above when you only want to export your model.*** ","metadata":{}},{"cell_type":"code","source":"#Model export. \n#Use '--streaming' flag to export a model capable of real time processing.\n#Use '--stereo' for a model with two decoders.\n\n#Copy contents of earlier training to /kaggle/working folder.\n!cp -r /kaggle/input/root_folder_of_your_earlier_training/* /kaggle/working\n\n#Export model\n!/kaggle/temp/miniconda/bin/rave export \\\n--run /kaggle/working/runs/your_training_name_with_a_random_string_in_the_end \\\n--streaming \\\n--stereo","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}